{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查Python版本\n",
    "from sys import version_info\n",
    "if version_info.major != 3:\n",
    "    \n",
    "    raise Exception('请使用Python3来完成此项目')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取数据\n",
    "[Dogs vs. Cats Redux: Kernels Edition\n",
    "](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file is exist, no need download\n",
      "files found\n",
      "file is exist, no need download\n",
      "files found\n"
     ]
    }
   ],
   "source": [
    "# download data and unzip it\n",
    "# from urllib.request import urlretrieve\n",
    "import subprocess\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "train_url = ['kaggle','competitions','download','-c','dogs-vs-cats-redux-kernels-edition',\n",
    "            '-f','train.zip','-p','./']\n",
    "test_url = ['kaggle','competitions','download','-c','dogs-vs-cats-redux-kernels-edition',\n",
    "            '-f','test.zip','-p','./']\n",
    "sample_csv_url = ['kaggle','competitions','download','-c','dogs-vs-cats-redux-kernels-edition',\n",
    "                  '-f','sample_submission.csv','-p','./']\n",
    "\n",
    "\n",
    "def download_unzip_dataset(url, zip_file_path, folder_path, unzip=True):\n",
    "    if os.path.exists(zip_file_path):\n",
    "        print(\"file is exist, no need download\")\n",
    "    else:\n",
    "        print(\"download now\")\n",
    "#         urlretrieve(url, zip_file_path)\n",
    "        subp = subprocess.run(url)\n",
    "#         subp.wait()\n",
    "    \n",
    "    if unzip:\n",
    "        if os.path.exists(folder_path):\n",
    "            print(\"files found\")\n",
    "        else:\n",
    "            print(\"unzip now\")\n",
    "            zipf = ZipFile(zip_file_path)\n",
    "            zipf.extractall()\n",
    "            print(\"unzip end\")\n",
    "\n",
    "download_unzip_dataset(train_url, 'train.zip', 'train/')\n",
    "download_unzip_dataset(test_url, 'test.zip', 'test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分离数据集，dog和cat图片分别放入train2/dogs, train2/cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split over\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def split_train_set(old_dir, new_dir):\n",
    "    file_list = os.listdir(old_dir)\n",
    "    file_cats = filter(lambda x:x[:3] == 'cat', file_list)\n",
    "    file_dogs = filter(lambda x:x[:3] == 'dog', file_list)\n",
    "\n",
    "    if os.path.exists(new_dir):\n",
    "        shutil.rmtree(new_dir)\n",
    "    os.mkdir(new_dir)\n",
    "    \n",
    "    dogs_path = os.path.join(new_dir, 'dogs')\n",
    "    cats_path = os.path.join(new_dir, 'cats')\n",
    "    os.mkdir(dogs_path)\n",
    "    os.mkdir(cats_path)\n",
    "    \n",
    "    # 此处要注意： os.symlink(src, dst)\n",
    "    # dst是从它所在的目录去选择src,所以src必须是相对于dst的relative path\n",
    "    for filename in file_cats:\n",
    "        os.symlink('../../'+old_dir+filename, cats_path+'/'+filename)\n",
    "    \n",
    "    for filename in file_dogs:\n",
    "        os.symlink('../../'+old_dir+filename, dogs_path+'/'+filename)\n",
    "        \n",
    "    print(\"split over\")\n",
    "\n",
    "split_train_set('train/', 'pre-train/')\n",
    "\n",
    "# preprocess test image folder\n",
    "if os.path.exists('pre-test'):\n",
    "    shutil.rmtree('pre-test')\n",
    "os.mkdir('pre-test')\n",
    "os.symlink('../test', 'pre-test/test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取特征\n",
    "\n",
    "利用pre-trained model提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备训练集和测试集\n",
    "对于样本数非常多的数据集，可以利用generator函数来减少计算的次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取特征\n",
    "- 利用 pre-trained 模型从train/test dataset中提取出特征，然后使用自定义的fully-connected层在这些提取的特征集上训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为了增加代码复用，方便调试其它的pre-trained model，将上面的2个步骤封装为一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import *\n",
    "from keras.applications import resnet50\n",
    "from keras.applications import xception\n",
    "from keras.applications import inception_v3\n",
    "from keras.applications import inception_resnet_v2\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "import h5py\n",
    "\n",
    "def get_pre_features_from_images(MODEL, image_size, preprocess_input, model_name):\n",
    "    # create ImageDataGenerator, and indicate \"preprocessing_functions\"\n",
    "    image_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    \n",
    "    train_generator = image_gen.flow_from_directory('pre-train', \n",
    "                                                target_size=image_size, \n",
    "                                                shuffle=False, # our data will be in order\n",
    "                                                batch_size=16)\n",
    "    \n",
    "    test_generator = image_gen.flow_from_directory('pre-test', \n",
    "                                               target_size=image_size, \n",
    "                                               shuffle=False, # our data will be in order\n",
    "                                               batch_size=16, \n",
    "                                               class_mode=None, # this means our generator will only yield batches of data, no labels\n",
    "                                              )\n",
    "    \n",
    "    ## use pre-trained model to get features from image generator\n",
    "    x = Input((image_size[0], image_size[1], 3)) # shape: width, height, channel\n",
    "    base_model = MODEL(input_tensor=x, weights='imagenet', include_top=False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    \n",
    "    # the predict_generator method returns the output of a model, given\n",
    "    # a generator that yields batches of numpy data\n",
    "    pre_features_train = model.predict_generator(train_generator, verbose=1)\n",
    "    pre_features_test = model.predict_generator(test_generator, verbose=1)\n",
    "    \n",
    "    # save the output to h5 file\n",
    "    out_filename = model_name + \"_pre_out.h5\"\n",
    "    with h5py.File(out_filename) as h:\n",
    "        h.create_dataset(\"train\", data=pre_features_train)\n",
    "        h.create_dataset(\"label\", data=train_generator.classes)\n",
    "        h.create_dataset(\"test\", data=pre_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "1563/1563 [==============================] - 85s 54ms/step\n",
      "782/782 [==============================] - 47s 60ms/step\n"
     ]
    }
   ],
   "source": [
    "get_pre_features_from_images(resnet50.ResNet50, (224,224), resnet50.preprocess_input, \"ResNet50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "1563/1563 [==============================] - 98s 63ms/step\n",
      "782/782 [==============================] - 49s 62ms/step\n"
     ]
    }
   ],
   "source": [
    "get_pre_features_from_images(xception.Xception, (299,299), xception.preprocess_input, \"Xception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 1s 0us/step\n",
      "1563/1563 [==============================] - 92s 59ms/step\n",
      "782/782 [==============================] - 43s 55ms/step\n"
     ]
    }
   ],
   "source": [
    "get_pre_features_from_images(inception_v3.InceptionV3, (299,299), inception_v3.preprocess_input, \"InceptionV3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pre_features_from_images(inception_resnet_v2.InceptionResNetV2, (299,299), inception_resnet_v2.preprocess_input,\"InceptionResNetV2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
