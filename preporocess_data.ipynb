{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查Python版本\n",
    "from sys import version_info\n",
    "if version_info.major != 3:\n",
    "    \n",
    "    raise Exception('请使用Python3来完成此项目')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取数据\n",
    "[Dogs vs. Cats Redux: Kernels Edition\n",
    "](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file is exist, no need download\n",
      "files found\n",
      "file is exist, no need download\n",
      "unzip now\n",
      "unzip end\n"
     ]
    }
   ],
   "source": [
    "# download data and unzip it\n",
    "# from urllib.request import urlretrieve\n",
    "import subprocess\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "train_url = ['kaggle','competitions','download','-c','dogs-vs-cats-redux-kernels-edition',\n",
    "            '-f','train.zip','-p','./']\n",
    "test_url = ['kaggle','competitions','download','-c','dogs-vs-cats-redux-kernels-edition',\n",
    "            '-f','test.zip','-p','./']\n",
    "sample_csv_url = ['kaggle','competitions','download','-c','dogs-vs-cats-redux-kernels-edition',\n",
    "                  '-f','sample_submission.csv','-p','./']\n",
    "\n",
    "\n",
    "def download_unzip_dataset(url, zip_file_path, folder_path, unzip=True):\n",
    "    if os.path.exists(zip_file_path):\n",
    "        print(\"file is exist, no need download\")\n",
    "    else:\n",
    "        print(\"download now\")\n",
    "#         urlretrieve(url, zip_file_path)\n",
    "        subp = subprocess.run(url)\n",
    "#         subp.wait()\n",
    "    \n",
    "    if unzip:\n",
    "        if os.path.exists(folder_path):\n",
    "            print(\"files found\")\n",
    "        else:\n",
    "            print(\"unzip now\")\n",
    "            zipf = ZipFile(zip_file_path)\n",
    "            zipf.extractall()\n",
    "            print(\"unzip end\")\n",
    "\n",
    "download_unzip_dataset(train_url, 'train.zip', 'train/')\n",
    "download_unzip_dataset(test_url, 'test.zip', 'test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分离数据集，dog和cat图片分别放入train2/dogs, train2/cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split over\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def split_train_set(old_dir, new_dir):\n",
    "    file_list = os.listdir(old_dir)\n",
    "    file_cats = filter(lambda x:x[:3] == 'cat', file_list)\n",
    "    file_dogs = filter(lambda x:x[:3] == 'dog', file_list)\n",
    "\n",
    "    if os.path.exists(new_dir):\n",
    "        shutil.rmtree(new_dir)\n",
    "    os.mkdir(new_dir)\n",
    "    \n",
    "    dogs_path = os.path.join(new_dir, 'dogs')\n",
    "    cats_path = os.path.join(new_dir, 'cats')\n",
    "    os.mkdir(dogs_path)\n",
    "    os.mkdir(cats_path)\n",
    "    \n",
    "    # 此处要注意： os.symlink(src, dst)\n",
    "    # dst是从它所在的目录去选择src,所以src必须是相对于dst的relative path\n",
    "    for filename in file_cats:\n",
    "        os.symlink('../../'+old_dir+filename, cats_path+'/'+filename)\n",
    "    \n",
    "    for filename in file_dogs:\n",
    "        os.symlink(old_dir+filename, dogs_path+'/'+filename)\n",
    "        \n",
    "    print(\"split over\")\n",
    "\n",
    "split_train_set('train/', 'pre-train/')\n",
    "\n",
    "# preprocess test image folder\n",
    "if os.path.exists('pre-test'):\n",
    "    shutil.rmtree('pre-test')\n",
    "os.mkdir('pre-test')\n",
    "os.symlink('../test', 'pre-test/test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取特征\n",
    "\n",
    "利用pre-trained model提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备训练集和测试集\n",
    "对于样本数非常多的数据集，可以利用generator函数来减少计算的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import *\n",
    "\n",
    "image_size = (224,224)\n",
    "\n",
    "image_gen = ImageDataGenerator()\n",
    "train_generator = image_gen.flow_from_directory('pre-train', \n",
    "                                                target_size=image_size, \n",
    "                                                shuffle=False, # our data will be in order\n",
    "                                                batch_size=16)\n",
    "test_generator = image_gen.flow_from_directory('pre-test', \n",
    "                                               target_size=image_size, \n",
    "                                               shuffle=False, # our data will be in order\n",
    "                                               batch_size=16, \n",
    "                                               class_mode=None, # this means our generator will only yield batches of data, no labels\n",
    "                                              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取特征\n",
    "- 利用 pre-trained 模型从train/test dataset中提取出特征，然后使用自定义的fully-connected层在这些提取的特征集上训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import resnet50\n",
    "import h5py\n",
    "\n",
    "x = Input((image_size[0], image_size[1], 3)) # shape: width, height, channel\n",
    "base_model = resnet50.ResNet50(input_tensor=resnet50.preprocess_input(x) weights='imagenet')\n",
    "\n",
    "# the predict_generator method returns the output of a model, given\n",
    "# a generator that yields batches of numpy data\n",
    "bottleneck_features_train = base_model.predict_generator(train_generator, train_generator.nb_sample)\n",
    "# save the output as a Numpy array\n",
    "# np.save(open('bottleneck_features_train.npy', 'w'), bottleneck_features_train)\n",
    "\n",
    "bottleneck_features_test = model.predict_generator(test_generator, test_generator.nb_sample)\n",
    "# np.save(open('bottleneck_features_test.npy', 'w'), bottleneck_features_test)\n",
    "\n",
    "with h5py.File(\"pre_out\") as h:\n",
    "        h.create_dataset(\"train\", data=bottleneck_features_train)\n",
    "        h.create_dataset(\"label\", data=train_generator.classes)\n",
    "        h.create_dataset(\"test\", data=bottleneck_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为了增加代码复用，方便调试其它的pre-trained model，将上面的2个步骤封装为一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import *\n",
    "from keras.applications import resnet50\n",
    "from keras.applications import xception\n",
    "from keras.applications import inception_v3\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "import h5py\n",
    "\n",
    "def get_pre_features_from_images(MODEL, image_size, preprocess_input, model_name):\n",
    "    image_gen = ImageDataGenerator()\n",
    "    train_generator = image_gen.flow_from_directory('pre-train', \n",
    "                                                target_size=image_size, \n",
    "                                                shuffle=False, # our data will be in order\n",
    "                                                batch_size=16)\n",
    "    \n",
    "    test_generator = image_gen.flow_from_directory('pre-test', \n",
    "                                               target_size=image_size, \n",
    "                                               shuffle=False, # our data will be in order\n",
    "                                               batch_size=16, \n",
    "                                               class_mode=None, # this means our generator will only yield batches of data, no labels\n",
    "                                              )\n",
    "    \n",
    "    ## use pre-trained model to get features from image generator\n",
    "    x = Input((image_size[0], image_size[1], 3)) # shape: width, height, channel\n",
    "    x = preprocess_input(x)\n",
    "    base_model = MODEL(input_tensor=x, weights='imagenet')\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    \n",
    "    # the predict_generator method returns the output of a model, given\n",
    "    # a generator that yields batches of numpy data\n",
    "    pre_features_train = model.predict_generator(train_generator, train_generator.nb_sample)\n",
    "    pre_features_test = model.predict_generator(test_generator, test_generator.nb_sample)\n",
    "    \n",
    "    # save the output to h5 file\n",
    "    out_filename = model_name + \"_pre_out.h5\"\n",
    "    with h5py.File(out_filename) as h:\n",
    "        h.create_dataset(\"train\", data=pre_features_train)\n",
    "        h.create_dataset(\"label\", data=train_generator.classes)\n",
    "        h.create_dataset(\"test\", data=pre_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer global_average_pooling2d_1: expected ndim=4, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bc28eaf81d3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_pre_features_from_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ResNet50\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-d7903f5e3c4d>\u001b[0m in \u001b[0;36mget_pre_features_from_images\u001b[0;34m(MODEL, image_size, preprocess_input, model_name)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# the predict_generator method returns the output of a model, given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer global_average_pooling2d_1: expected ndim=4, found ndim=2"
     ]
    }
   ],
   "source": [
    "get_pre_features_from_images(resnet50.ResNet50, (224,224), resnet50.preprocess_input, \"ResNet50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pre_features_from_images(xception.Xception, (224,224), resnet50.preprocess_input, \"Xception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pre_features_from_images(inception_v3.InceptionV3, (224,224), resnet50.preprocess_input, \"InceptionV3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
